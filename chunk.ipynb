{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee85dc06-2391-479f-8b91-9a0e53fe74ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:33:36.827473Z",
     "iopub.status.busy": "2024-08-03T07:33:36.827164Z",
     "iopub.status.idle": "2024-08-03T07:33:36.835134Z",
     "shell.execute_reply": "2024-08-03T07:33:36.834073Z",
     "shell.execute_reply.started": "2024-08-03T07:33:36.827454Z"
    }
   },
   "source": [
    "# Part 2: Scrape and chunk text from URLs\n",
    "\n",
    "This notebook illustrates how to perform chunking on text, which has been scraped from specific URLs.\n",
    "\n",
    "Presentation for GraphGeeks.org on 2024-08-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bddacd-65ec-467a-accb-f7eaa76be54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T17:20:07.791255Z",
     "iopub.status.busy": "2024-08-03T17:20:07.790865Z",
     "iopub.status.idle": "2024-08-03T17:20:08.727639Z",
     "shell.execute_reply": "2024-08-03T17:20:08.727406Z",
     "shell.execute_reply.started": "2024-08-03T17:20:07.791233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from icecream import ic\n",
    "import requests\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914b6c41-62e7-4c61-ad12-a0cc88972ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T17:20:08.728439Z",
     "iopub.status.busy": "2024-08-03T17:20:08.728331Z",
     "iopub.status.idle": "2024-08-03T17:20:09.968735Z",
     "shell.execute_reply": "2024-08-03T17:20:09.968330Z",
     "shell.execute_reply.started": "2024-08-03T17:20:08.728431Z"
    }
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE: int = 1024\n",
    "\n",
    "SPACY_MODEL: str = \"en_core_web_md\"\n",
    "\n",
    "nlp: spacy.Language = spacy.load(SPACY_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7526cd-4350-4942-8681-f1ba06d5ee19",
   "metadata": {},
   "source": [
    "What is the ideal text chunk size? \n",
    "See <https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4db8fa1-25e0-492f-807f-0aa0ce2f6373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T17:20:09.969385Z",
     "iopub.status.busy": "2024-08-03T17:20:09.969236Z",
     "iopub.status.idle": "2024-08-03T17:20:09.972785Z",
     "shell.execute_reply": "2024-08-03T17:20:09.972340Z",
     "shell.execute_reply.started": "2024-08-03T17:20:09.969379Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_chunk (\n",
    "    doc: spacy.tokens.doc.Doc,\n",
    "    chunk_id: int,\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "Split the given document into text chunks, returning the last index.\n",
    "    \"\"\"\n",
    "    chunks: typing.List[ str ] = []\n",
    "    chunk_total: int = 0\n",
    "    prev_line: str = \"\"\n",
    "\n",
    "    for sent_id, sent in enumerate(doc.sents):\n",
    "        line: str = str(sent).strip()\n",
    "        line_len: int = len(line)\n",
    "    \n",
    "        if (chunk_total + line_len) > CHUNK_SIZE:\n",
    "            # emit current chunk\n",
    "            print(\"--- chunk_id: \", chunk_id)\n",
    "            print(\"\\n\".join(chunks))\n",
    "            print()\n",
    "\n",
    "            # make a new chunk\n",
    "            chunks = [ prev_line, line ]\n",
    "            chunk_total = len(prev_line) + line_len\n",
    "            chunk_id += 1\n",
    "        else:\n",
    "            # append to current chunk\n",
    "            chunks.append(line)\n",
    "            chunk_total += line_len\n",
    "\n",
    "        prev_line = line\n",
    "\n",
    "    # emit last chunk\n",
    "    print(\"--- chunk_id: \", chunk_id)\n",
    "    print(\"\\n\".join(chunks))\n",
    "\n",
    "    return chunk_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84646da-28ec-448c-882d-31f5a8f4934d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T17:20:09.973216Z",
     "iopub.status.busy": "2024-08-03T17:20:09.973142Z",
     "iopub.status.idle": "2024-08-03T17:20:10.608121Z",
     "shell.execute_reply": "2024-08-03T17:20:10.607836Z",
     "shell.execute_reply.started": "2024-08-03T17:20:09.973207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- chunk_id:  0\n",
      "US researchers say they have uncovered potential link after tracking 130,000 people over four decades\n",
      "Eating processed red meat could be a significant risk factor for dementia, according to a large study that tracked more than 100,000 people over four decades.\n",
      "Processed red meat has previously been shown to increase the risk of cancer, heart disease and type 2 diabetes.\n",
      "Now US researchers say they have uncovered a potential link to dementia.\n",
      "The study also found that replacing processed red meat with healthier foods such as nuts, beans or tofu could help reduce the risk of dementia.\n",
      "The findings were presented at the Alzheimer’s Association international conference in the US.\n",
      "The number of people living with dementia globally is forecast to nearly triple to 153 million by 2050, and studies looking at diet and risk of cognitive decline has become a focus of researchers.\n",
      "In the latest research, experts studied the health of 130,000 nurses and other health workers working in the US.\n",
      "\n",
      "--- chunk_id:  1\n",
      "In the latest research, experts studied the health of 130,000 nurses and other health workers working in the US.\n",
      "They were tracked for 43 years and provided data on their diet every 2 to 5 years.\n",
      "The participants were asked how often they ate processed red meat including bacon, hotdogs, sausages, salami and other sandwich meat.\n",
      "They were also asked about their consumption of nuts and legumes including peanut butter, peanuts, walnuts and other nuts, string beans, beans, peas, soy milk and tofu.\n",
      "More than 11,000 cases of dementia were identified during the follow-up period.\n",
      "Consuming two servings of processed red meat each week appeared to raise the risk of cognitive decline by 14% compared with those eating about three servings a month, the researchers reported.\n",
      "The study also suggested that replacing one daily serving of processed red meat for a daily serving of nuts, beans or tofu every day could lower the risk of dementia by 23%.\n",
      "\n",
      "--- chunk_id:  2\n",
      "The study also suggested that replacing one daily serving of processed red meat for a daily serving of nuts, beans or tofu every day could lower the risk of dementia by 23%.\n",
      "The lead author of the study, Dr Yuhan Li, an assistant professor at the Brigham and Women’s hospital in Boston, said: “Study results have been mixed on whether there is a relationship between cognitive decline and meat consumption in general, so we took a closer look at how eating different amounts of both processed and unprocessed meat affects cognitive risk and function.\n",
      "“By studying people over a long period of time, we found that eating processed red meat could be a significant risk factor for dementia.”\n",
      "Li, who conducted the study while at the Harvard TH Chan school of public health in Boston, added: “Dietary guidelines could include recommendations limiting it to promote brain health.\n",
      "“Processed red meat has also been shown to raise the risk of cancer, heart disease and diabetes.\n",
      "\n",
      "--- chunk_id:  3\n",
      "“Processed red meat has also been shown to raise the risk of cancer, heart disease and diabetes.\n",
      "It may affect the brain because it has high levels of harmful substances such as nitrites [preservatives] and sodium.”\n",
      "Dr Heather Snyder, of the Alzheimer’s Association, said: “Prevention of Alzheimer’s disease and all other dementia is a major focus, and the Alzheimer’s Association has long encouraged eating a healthier diet – including foods that are less processed – because they’ve been associated with lower the risk of cognitive decline.\n",
      "This large, long-term study provides a specific example of one way to eat healthier.”\n",
      "Dr Richard Oakley, of the Alzheimer’s Society in the UK, said: “In this study more people who ate processed red meat went on to develop dementia and had worse memory and thinking skills.”\n",
      "However, he urged caution because the research found only an association between processed red meat and dementia – and did not prove cause and effect.\n",
      "\n",
      "--- chunk_id:  4\n",
      "This large, long-term study provides a specific example of one way to eat healthier.”\n",
      "Dr Richard Oakley, of the Alzheimer’s Society in the UK, said: “In this study more people who ate processed red meat went on to develop dementia and had worse memory and thinking skills.”\n",
      "However, he urged caution because the research found only an association between processed red meat and dementia – and did not prove cause and effect.\n",
      "“It’s important to remember that this doesn’t mean that eating processed red meat is directly related to developing dementia.\n",
      "It may be that people who avoid processed red meat are generally more health conscious and avoid other unhealthy habits that increase dementia risk.”\n"
     ]
    }
   ],
   "source": [
    "chunk_id: int = 0\n",
    "\n",
    "url_list: typing.List[ str ] = [\n",
    "    \"https://www.theguardian.com/society/article/2024/jul/31/eating-processed-red-meat-could-increase-risk-of-dementia-study-finds\",\n",
    "]\n",
    "\n",
    "\n",
    "for url in url_list:\n",
    "    response: requests.Response = requests.get(url)\n",
    "    soup: BeautifulSoup = BeautifulSoup(response.text)\n",
    "\n",
    "    doc: spacy.tokens.doc.Doc = nlp(\"\\n\".join([\n",
    "        para.text.strip()\n",
    "        for para in soup.findAll(\"p\")\n",
    "    ]))\n",
    "\n",
    "    chunk_id = make_chunk(doc, chunk_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
